{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SummAI - Business Summarization Project\n",
    "## Extractive vs Abstractive Summarization Comparison\n",
    "\n",
    "**Dataset**: CNN/DailyMail  \n",
    "**Methods**: TextRank (Extractive) vs BART (Abstractive)  \n",
    "**Evaluation**: ROUGE Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download NLTK Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"âœ“ NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load CNN/DailyMail Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading CNN/DailyMail dataset...\")\n",
    "print(\"This may take a few minutes on first run...\\n\")\n",
    "\n",
    "# Load a sample of CNN/DailyMail dataset\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1000]\")\n",
    "test_data = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:100]\")\n",
    "\n",
    "print(f\"âœ“ Training samples: {len(dataset)}\")\n",
    "print(f\"âœ“ Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a sample\n",
    "sample = dataset[0]\n",
    "\n",
    "print(\"Sample Article (first 500 chars):\")\n",
    "print(\"=\"*60)\n",
    "print(sample['article'][:500] + \"...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Reference Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(sample['highlights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Extractive Summarizer (TextRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractiveSummarizer:\n",
    "    \"\"\"TextRank-based extractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Clean and tokenize text\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        return sentences\n",
    "    \n",
    "    def calculate_similarity(self, sent1, sent2):\n",
    "        \"\"\"Calculate similarity between two sentences using TF-IDF\"\"\"\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform([sent1, sent2])\n",
    "            similarity = (tfidf_matrix * tfidf_matrix.T).toarray()[0][1]\n",
    "            return similarity\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Generate extractive summary using TextRank algorithm\"\"\"\n",
    "        sentences = self.preprocess(text)\n",
    "        \n",
    "        if len(sentences) <= num_sentences:\n",
    "            return ' '.join(sentences)\n",
    "        \n",
    "        # Build similarity matrix\n",
    "        similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            for j in range(len(sentences)):\n",
    "                if i != j:\n",
    "                    similarity_matrix[i][j] = self.calculate_similarity(\n",
    "                        sentences[i], sentences[j]\n",
    "                    )\n",
    "        \n",
    "        # Apply PageRank algorithm\n",
    "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        \n",
    "        # Rank sentences\n",
    "        ranked_sentences = sorted(\n",
    "            ((scores[i], s) for i, s in enumerate(sentences)), \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Select top sentences\n",
    "        summary_sentences = sorted(\n",
    "            ranked_sentences[:num_sentences], \n",
    "            key=lambda x: sentences.index(x[1])\n",
    "        )\n",
    "        \n",
    "        summary = ' '.join([s[1] for s in summary_sentences])\n",
    "        return summary\n",
    "\n",
    "print(\"âœ“ Extractive Summarizer class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Abstractive Summarizer (BART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractiveSummarizer:\n",
    "    \"\"\"BART-based abstractive summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"facebook/bart-large-cnn\"):\n",
    "        print(f\"Loading {model_name} model...\")\n",
    "        print(\"This may take a few minutes on first run...\")\n",
    "        self.summarizer = pipeline(\"summarization\", model=model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        print(\"âœ“ Model loaded successfully!\")\n",
    "    \n",
    "    def summarize(self, text, max_length=130, min_length=30):\n",
    "        \"\"\"Generate abstractive summary\"\"\"\n",
    "        # Truncate if text is too long\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "        \n",
    "        summary = self.summarizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        return summary[0]['summary_text']\n",
    "\n",
    "print(\"âœ“ Abstractive Summarizer class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing models...\\n\")\n",
    "\n",
    "extractive_model = ExtractiveSummarizer()\n",
    "print(\"âœ“ Extractive model ready!\\n\")\n",
    "\n",
    "abstractive_model = AbstractiveSummarizer()\n",
    "print(\"\\nâœ“ Both models initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Both Methods on Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a test article\n",
    "test_article = test_data[0]['article']\n",
    "reference_summary = test_data[0]['highlights']\n",
    "\n",
    "print(\"Original Article (first 400 chars):\")\n",
    "print(\"=\"*60)\n",
    "print(test_article[:400] + \"...\\n\")\n",
    "\n",
    "print(\"Reference Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(reference_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate extractive summary\n",
    "print(\"Generating EXTRACTIVE summary...\\n\")\n",
    "extractive_summary = extractive_model.summarize(test_article, num_sentences=3)\n",
    "\n",
    "print(\"Extractive Summary (TextRank):\")\n",
    "print(\"=\"*60)\n",
    "print(extractive_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate abstractive summary\n",
    "print(\"Generating ABSTRACTIVE summary...\\n\")\n",
    "abstractive_summary = abstractive_model.summarize(test_article)\n",
    "\n",
    "print(\"Abstractive Summary (BART):\")\n",
    "print(\"=\"*60)\n",
    "print(abstractive_summary)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define Evaluation Metrics (ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_summary(reference, summary):\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, summary)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Evaluation function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating models on test set...\")\n",
    "print(\"This will take 10-15 minutes...\\n\")\n",
    "\n",
    "results = {\n",
    "    'extractive': {'rouge1': [], 'rouge2': [], 'rougeL': []},\n",
    "    'abstractive': {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "}\n",
    "\n",
    "# Test on first 20 samples for comparison\n",
    "num_test_samples = 20\n",
    "\n",
    "for i, sample in enumerate(test_data[:num_test_samples]):\n",
    "    article = sample['article']\n",
    "    reference = sample['highlights']\n",
    "    \n",
    "    # Extractive summary\n",
    "    ext_summary = extractive_model.summarize(article, num_sentences=3)\n",
    "    ext_scores = evaluate_summary(reference, ext_summary)\n",
    "    \n",
    "    # Abstractive summary\n",
    "    abs_summary = abstractive_model.summarize(article)\n",
    "    abs_scores = evaluate_summary(reference, abs_summary)\n",
    "    \n",
    "    # Store scores\n",
    "    for key in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        results['extractive'][key].append(ext_scores[key])\n",
    "        results['abstractive'][key].append(abs_scores[key])\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"âœ“ Processed {i+1}/{num_test_samples} samples...\")\n",
    "\n",
    "print(\"\\nâœ“ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method in ['extractive', 'abstractive']:\n",
    "    print(f\"\\n{method.upper()} SUMMARIZATION:\")\n",
    "    print(\"-\" * 40)\n",
    "    for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
    "        avg_score = np.mean(results[method][metric])\n",
    "        print(f\"  {metric.upper():10s}: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data for visualization\n",
    "metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n",
    "extractive_scores = [\n",
    "    np.mean(results['extractive']['rouge1']),\n",
    "    np.mean(results['extractive']['rouge2']),\n",
    "    np.mean(results['extractive']['rougeL'])\n",
    "]\n",
    "abstractive_scores = [\n",
    "    np.mean(results['abstractive']['rouge1']),\n",
    "    np.mean(results['abstractive']['rouge2']),\n",
    "    np.mean(results['abstractive']['rougeL'])\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, extractive_scores, width, label='Extractive', color='#667eea')\n",
    "bars2 = ax.bar(x + width/2, abstractive_scores, width, label='Abstractive', color='#764ba2')\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Scores', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Extractive vs Abstractive Summarization Performance', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Extractive (TextRank)', 'Abstractive (BART)'],\n",
    "    'ROUGE-1': [np.mean(results['extractive']['rouge1']), \n",
    "                np.mean(results['abstractive']['rouge1'])],\n",
    "    'ROUGE-2': [np.mean(results['extractive']['rouge2']), \n",
    "                np.mean(results['abstractive']['rouge2'])],\n",
    "    'ROUGE-L': [np.mean(results['extractive']['rougeL']), \n",
    "                np.mean(results['abstractive']['rougeL'])]\n",
    "})\n",
    "\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Models for Web Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "print(\"Saving models for web application...\\n\")\n",
    "\n",
    "# Save extractive model\n",
    "with open('extractive_model.pkl', 'wb') as f:\n",
    "    pickle.dump(extractive_model, f)\n",
    "\n",
    "print(\"âœ“ Extractive model saved as 'extractive_model.pkl'\")\n",
    "print(\"âœ“ Abstractive model will be loaded directly in web app\")\n",
    "print(\"\\nðŸŽ‰ All done! Models ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test with Custom Business Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with your own business text\n",
    "custom_text = \"\"\"\n",
    "Tesla Inc. reported record quarterly revenue on Wednesday, beating Wall Street expectations \n",
    "as the electric vehicle maker delivered more cars than anticipated. The company's revenue \n",
    "reached $25.2 billion in the fourth quarter, up 37% from a year earlier. Net income rose \n",
    "to $3.7 billion. CEO Elon Musk said the company expects to produce 2 million vehicles in 2024, \n",
    "doubling its current production capacity. The strong results come despite concerns about \n",
    "increasing competition in the EV market and economic headwinds. Tesla's stock price jumped \n",
    "8% in after-hours trading following the announcement.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Testing with custom business text:\\n\")\n",
    "print(\"Original Text:\")\n",
    "print(\"=\"*60)\n",
    "print(custom_text.strip())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "print(\"\\nExtractive Summary:\")\n",
    "print(\"-\"*60)\n",
    "print(extractive_model.summarize(custom_text, num_sentences=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAbstractive Summary:\")\n",
    "print(\"-\"*60)\n",
    "print(abstractive_model.summarize(custom_text, max_length=100, min_length=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Project Complete!\n",
    "\n",
    "### What We Built:\n",
    "1. âœ… Extractive Summarization (TextRank Algorithm)\n",
    "2. âœ… Abstractive Summarization (BART Model)\n",
    "3. âœ… Evaluation on CNN/DailyMail Dataset\n",
    "4. âœ… ROUGE Score Comparison\n",
    "5. âœ… Models Saved for Web Application\n",
    "\n",
    "### Next Steps:\n",
    "- Run `python app.py` to start the web application\n",
    "- Visit http://localhost:5000 in your browser\n",
    "- Test both summarization methods on business documents!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}